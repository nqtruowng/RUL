{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2934a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7216\\2654642488.py:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df = pd.read_csv('test_FD003.txt', sep='\\s+', header=None, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Đã chuyển đổi thành công!\n",
      "✓ File test_FD003.csv có 16,596 dòng và 26 cột\n",
      "\n",
      "Các cột: unit_number, time_cycles, op_setting_1, op_setting_2, op_setting_3, sensor_1, sensor_2, sensor_3, sensor_4, sensor_5, sensor_6, sensor_7, sensor_8, sensor_9, sensor_10, sensor_11, sensor_12, sensor_13, sensor_14, sensor_15, sensor_16, sensor_17, sensor_18, sensor_19, sensor_20, sensor_21\n",
      "\n",
      "5 dòng đầu tiên:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_number</th>\n",
       "      <th>time_cycles</th>\n",
       "      <th>op_setting_1</th>\n",
       "      <th>op_setting_2</th>\n",
       "      <th>op_setting_3</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_12</th>\n",
       "      <th>sensor_13</th>\n",
       "      <th>sensor_14</th>\n",
       "      <th>sensor_15</th>\n",
       "      <th>sensor_16</th>\n",
       "      <th>sensor_17</th>\n",
       "      <th>sensor_18</th>\n",
       "      <th>sensor_19</th>\n",
       "      <th>sensor_20</th>\n",
       "      <th>sensor_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0017</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.94</td>\n",
       "      <td>1581.93</td>\n",
       "      <td>1396.93</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.89</td>\n",
       "      <td>2387.94</td>\n",
       "      <td>8133.48</td>\n",
       "      <td>8.3760</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.07</td>\n",
       "      <td>23.4468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.02</td>\n",
       "      <td>1584.86</td>\n",
       "      <td>1398.90</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.85</td>\n",
       "      <td>2388.01</td>\n",
       "      <td>8137.44</td>\n",
       "      <td>8.4062</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.04</td>\n",
       "      <td>23.4807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.68</td>\n",
       "      <td>1581.78</td>\n",
       "      <td>1391.92</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.10</td>\n",
       "      <td>2387.94</td>\n",
       "      <td>8138.25</td>\n",
       "      <td>8.3553</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.10</td>\n",
       "      <td>23.4244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.20</td>\n",
       "      <td>1584.53</td>\n",
       "      <td>1395.34</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.45</td>\n",
       "      <td>2387.96</td>\n",
       "      <td>8137.07</td>\n",
       "      <td>8.3709</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.97</td>\n",
       "      <td>23.4782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1589.03</td>\n",
       "      <td>1395.86</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.91</td>\n",
       "      <td>2387.97</td>\n",
       "      <td>8134.20</td>\n",
       "      <td>8.4146</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.09</td>\n",
       "      <td>23.3950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit_number  time_cycles  op_setting_1  op_setting_2  op_setting_3  \\\n",
       "0            1            1       -0.0017       -0.0004         100.0   \n",
       "1            1            2        0.0006       -0.0002         100.0   \n",
       "2            1            3        0.0014       -0.0003         100.0   \n",
       "3            1            4        0.0027        0.0001         100.0   \n",
       "4            1            5       -0.0001        0.0001         100.0   \n",
       "\n",
       "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  ...  sensor_12  \\\n",
       "0    518.67    641.94   1581.93   1396.93     14.62  ...     521.89   \n",
       "1    518.67    642.02   1584.86   1398.90     14.62  ...     521.85   \n",
       "2    518.67    641.68   1581.78   1391.92     14.62  ...     522.10   \n",
       "3    518.67    642.20   1584.53   1395.34     14.62  ...     522.45   \n",
       "4    518.67    642.46   1589.03   1395.86     14.62  ...     521.91   \n",
       "\n",
       "   sensor_13  sensor_14  sensor_15  sensor_16  sensor_17  sensor_18  \\\n",
       "0    2387.94    8133.48     8.3760       0.03        391       2388   \n",
       "1    2388.01    8137.44     8.4062       0.03        391       2388   \n",
       "2    2387.94    8138.25     8.3553       0.03        391       2388   \n",
       "3    2387.96    8137.07     8.3709       0.03        392       2388   \n",
       "4    2387.97    8134.20     8.4146       0.03        391       2388   \n",
       "\n",
       "   sensor_19  sensor_20  sensor_21  \n",
       "0      100.0      39.07    23.4468  \n",
       "1      100.0      39.04    23.4807  \n",
       "2      100.0      39.10    23.4244  \n",
       "3      100.0      38.97    23.4782  \n",
       "4      100.0      39.09    23.3950  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chuyển đổi test_FD003.txt sang CSV\n",
    "import pandas as pd\n",
    "\n",
    "# Định nghĩa tên các cột\n",
    "columns = ['unit_number', 'time_cycles', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
    "          [f'sensor_{i}' for i in range(1, 22)]\n",
    "\n",
    "# Đọc file txt\n",
    "df = pd.read_csv('test_FD003.txt', sep='\\s+', header=None, names=columns)\n",
    "\n",
    "# Lưu thành CSV\n",
    "df.to_csv('test_FD003.csv', index=False)\n",
    "\n",
    "print(f'✓ Đã chuyển đổi thành công!')\n",
    "print(f'✓ File test_FD003.csv có {len(df):,} dòng và {len(df.columns)} cột')\n",
    "print(f'\\nCác cột: {\", \".join(df.columns.tolist())}')\n",
    "print(f'\\n5 dòng đầu tiên:')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee84683",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5243e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Paths and sensor->subsystem mapping\n",
    "TRAIN_PATH = 'train_FD001.txt'\n",
    "TEST_PATH  = 'test_FD001.txt'\n",
    "TRUTH_PATH = 'RUL_FD001.txt'\n",
    "\n",
    "# Sensor to subsystem mapping provided by user\n",
    "SENSOR_TO_SUBSYSTEM = {\n",
    "    'sensor_1': ['Fan inlet temperature', 'Overall Inlet'],\n",
    "    'sensor_2': ['Fan inlet pressure (psia)', 'Overall Inlet'],\n",
    "    'sensor_3': ['HPC outlet pressure (psia)','High-Pressure Compressor (HPC)'],\n",
    "    'sensor_4': ['HPT outlet temperature','High-Pressure Turbine (HPT)'],\n",
    "    'sensor_5': ['LPC outlet temperature','Low-Pressure Compressor (LPC)'],\n",
    "    'sensor_6': ['Fan speed rpm','Fan'],\n",
    "    'sensor_7': ['Fan inlet pressure (psia)', 'Overall Inlet'],\n",
    "    'sensor_8': ['HPC outlet static pressure (psia)','High-Pressure Compressor (HPC)'],\n",
    "    'sensor_9': ['Corrected fan speed (rpm)','Fan'],\n",
    "    'sensor_10': ['Ratio of pressure (bypass)Fan', 'Overall'],\n",
    "    'sensor_11': ['Fuel flow (pph)','Combustion Chamber'],\n",
    "    'sensor_12': ['LPC outlet temperature (corrected)','Low-Pressure Compressor (LPC)'],\n",
    "    'sensor_13': ['HPT outlet temperature (corrected)','High-Pressure Turbine (HPT)'],\n",
    "    'sensor_14': ['Physical fan speed (rpm)','Fan'],\n",
    "    'sensor_15': ['Physical core engine speed (rpm)','High-Pressure Compressor (HPC) and High-Pressure Turbine (HPT)'],\n",
    "    'sensor_16': ['Bleed air (units)','Overall Engine System'],\n",
    "    'sensor_17': ['Fuel-air ratio','Combustion Chamber'],\n",
    "    'sensor_18': ['Thrust-specific fuel consumption (TSFC)','Overall Engine System'],\n",
    "    'sensor_19': ['Total temperature at LPC exit','Low-Pressure Compressor (LPC)'],\n",
    "    'sensor_20': ['Engine pressure ratio','Overall Engine System'],\n",
    "    'sensor_21': ['Total temperature at HPT exit ','High-Pressure Turbine (HPT)']\n",
    "}\n",
    "\n",
    "# 2. Load & preprocess data (features scaled; RUL kept in cycles)\n",
    "train = pd.read_csv(TRAIN_PATH, sep=' ', header=None)\n",
    "train.drop([26,27], axis=1, inplace=True)\n",
    "cols = ['engine_id','cycle'] + [f'op_setting_{i+1}' for i in range(3)] + [f'sensor_{i+1}' for i in range(21)]\n",
    "train.columns = cols\n",
    "\n",
    "rul_df = train.groupby('engine_id')['cycle'].max().reset_index()\n",
    "rul_df.columns = ['engine_id', 'max_cycle']\n",
    "train = train.merge(rul_df, on='engine_id')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "MAX_RUL = 125\n",
    "train['RUL'] = train['RUL'].clip(upper=MAX_RUL)\n",
    "\n",
    "feature_cols = train.columns[2:-1]\n",
    "scaler = MinMaxScaler()\n",
    "train[feature_cols] = scaler.fit_transform(train[feature_cols])\n",
    "\n",
    "# test & truth\n",
    "test = pd.read_csv(TEST_PATH, sep=' ', header=None)\n",
    "test.drop([26,27], axis=1, inplace=True)\n",
    "test.columns = cols\n",
    "\n",
    "truth = pd.read_csv(TRUTH_PATH, header=None)\n",
    "truth.columns = ['RUL']\n",
    "\n",
    "# scale test\n",
    "test[feature_cols] = scaler.transform(test[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sequence creation helpers\n",
    "SEQ_LEN = 30\n",
    "\n",
    "def create_sequences(df, seq_length=SEQ_LEN):\n",
    "    X, y = [], []\n",
    "    for eid in df['engine_id'].unique():\n",
    "        ed = df[df['engine_id']==eid].reset_index(drop=True)\n",
    "        for i in range(len(ed)-seq_length):\n",
    "            X.append(ed.iloc[i:i+seq_length][feature_cols].values)\n",
    "            y.append(ed.iloc[i+seq_length]['RUL'])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_sequences_test_last(df, seq_length=SEQ_LEN):\n",
    "    X, engine_ids = [], []\n",
    "    for eid in df['engine_id'].unique():\n",
    "        ed = df[df['engine_id']==eid].reset_index(drop=True)\n",
    "        if len(ed) >= seq_length:\n",
    "            X.append(ed.iloc[-seq_length:][feature_cols].values)\n",
    "            engine_ids.append(eid)\n",
    "    return np.array(X), np.array(engine_ids)\n",
    "\n",
    "X, y = create_sequences(train, SEQ_LEN)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, test_engine_ids = create_sequences_test_last(test, SEQ_LEN)\n",
    "true_rul_test = truth['RUL'].values[:len(test_engine_ids)]\n",
    "print('Shapes -', X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dataset & dataloader\n",
    "class RUL_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1,1)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "BATCH = 64\n",
    "train_loader = DataLoader(RUL_Dataset(X_train, y_train), batch_size=BATCH, shuffle=True)\n",
    "val_loader = DataLoader(RUL_Dataset(X_val, y_val), batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model definition (parameterize cnn_channels)\n",
    "class CNNTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, cnn_channels=64, d_model=128, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, cnn_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_channels, cnn_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.input_proj = nn.Linear(cnn_channels, d_model)\n",
    "        self.fc = nn.Sequential(nn.Linear(d_model,64), nn.ReLU(), nn.Dropout(0.2), nn.Linear(64,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)  # [B, F, T]\n",
    "        x = self.cnn(x)       # [B, C, T]\n",
    "        x = x.permute(0,2,1)  # [B, T, C]\n",
    "        x = self.input_proj(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Asymmetric loss and metrics\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def asymmetric_loss(pred, true, lambda_over=10.0):\n",
    "    base = mse_loss(pred, true)\n",
    "    over_pen = torch.relu(pred - true)\n",
    "    over_term = (over_pen**2).mean()\n",
    "    return base + lambda_over * over_term\n",
    "\n",
    "def compute_overestimation_rate(preds, trues):\n",
    "    over = (preds - trues) > 0\n",
    "    return float(over.sum()) / len(over)\n",
    "\n",
    "# Modified asymmetric loss to weight by true RUL\n",
    "def weighted_asymmetric_loss(pred, true, lambda_over=10.0, max_rul=125):\n",
    "    \"\"\"\n",
    "    - Penalizes overestimation (lambda_over)\n",
    "    - Gives higher weight to high RUL samples\n",
    "    \"\"\"\n",
    "    base = mse_loss(pred, true)\n",
    "    over_pen = torch.relu(pred - true)\n",
    "    over_term = (over_pen**2).mean()\n",
    "    \n",
    "    # Weight by true RUL (normalized)\n",
    "    weight = (true / max_rul).view(-1,1)  # shape [batch_size,1]\n",
    "    weighted_base = (weight * (pred - true)**2).mean()\n",
    "    \n",
    "    return weighted_base + lambda_over * over_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training & evaluation functions\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, lambda_over):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(Xb)\n",
    "        loss = asymmetric_loss(preds, yb, lambda_over=lambda_over)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item() * Xb.size(0)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "def train_one_epoch_weighted(model, loader, optimizer, lambda_over, max_rul=125):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(Xb)\n",
    "        loss = weighted_asymmetric_loss(preds, yb, lambda_over=lambda_over, max_rul=max_rul)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item() * Xb.size(0)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = model(Xb)\n",
    "            all_preds.append(preds.cpu().numpy().flatten())\n",
    "            all_trues.append(yb.cpu().numpy().flatten())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_trues = np.concatenate(all_trues)\n",
    "    val_mse = mean_squared_error(all_trues, all_preds)\n",
    "    over_rate = compute_overestimation_rate(all_preds, all_trues)\n",
    "    return val_mse, over_rate, all_preds, all_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426466df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Hyperparameter grid search\n",
    "search_space = {\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'lambda_over': [5.0, 10.0, 20.0],\n",
    "    'cnn_channels': [32, 64]\n",
    "}\n",
    "\n",
    "BEST_DIR = 'hp_search_results'\n",
    "os.makedirs(BEST_DIR, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in search_space['lr']:\n",
    "    for lambda_over in search_space['lambda_over']:\n",
    "        for cnn_ch in search_space['cnn_channels']:\n",
    "            print(f\"Running grid point lr={lr}, lambda_over={lambda_over}, cnn_ch={cnn_ch}\")\n",
    "            model = CNNTransformerModel(input_dim=len(feature_cols), cnn_channels=cnn_ch).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            EPOCHS = 25\n",
    "            best_val_obj = float('inf')\n",
    "            patience = 6\n",
    "            no_improve = 0\n",
    "            for epoch in range(EPOCHS):\n",
    "                train_obj = train_one_epoch_weighted(model, train_loader, optimizer, lambda_over)\n",
    "                val_mse, val_over, _, _ = evaluate(model, val_loader)\n",
    "                # objective combines MSE and over-rate penalty\n",
    "                val_obj = val_mse + 1000.0 * val_over\n",
    "                print(f\"  Epoch {epoch+1}/{EPOCHS} train_obj={train_obj:.4f} val_mse={val_mse:.4f} val_over={val_over:.3f} val_obj={val_obj:.4f}\")\n",
    "                if val_obj < best_val_obj:\n",
    "                    best_val_obj = val_obj\n",
    "                    torch.save(model.state_dict(), os.path.join(BEST_DIR, f\"best_lr{lr}_lam{lambda_over}_ch{cnn_ch}.pth\"))\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= patience:\n",
    "                        print(\"  Early stopping\")\n",
    "                        break\n",
    "            # after training, load best and evaluate\n",
    "            model.load_state_dict(torch.load(os.path.join(BEST_DIR, f\"best_lr{lr}_lam{lambda_over}_ch{cnn_ch}.pth\"), map_location=device))\n",
    "            val_mse, val_over, val_preds, val_trues = evaluate(model, val_loader)\n",
    "            results.append({'lr': lr, 'lambda_over': lambda_over, 'cnn_ch': cnn_ch,\n",
    "                            'val_mse': val_mse, 'val_over': val_over, 'val_obj': val_mse + 1000.0*val_over})\n",
    "            # save summary CSV per grid point\n",
    "            pd.DataFrame(results).to_csv(os.path.join(BEST_DIR, 'grid_search_summary.csv'), index=False)\n",
    "\n",
    "# print sorted results\n",
    "res_df = pd.DataFrame(results).sort_values('val_obj')\n",
    "print('Top grid results:')\n",
    "print(res_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Load best hyperparameters & final evaluation on test\n",
    "best = res_df.iloc[0]\n",
    "print('Selected best config:', best.to_dict())\n",
    "best_model_path = os.path.join(BEST_DIR, f\"best_lr{best['lr']}_lam{best['lambda_over']}_ch{int(best['cnn_ch'])}.pth\")\n",
    "model = CNNTransformerModel(input_dim=len(feature_cols), cnn_channels=int(best['cnn_ch'])).to(device)\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "# test predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_pred_test = model(X_test_t).cpu().numpy().flatten()\n",
    "\n",
    "rmse_test = math.sqrt(mean_squared_error(true_rul_test, y_pred_test))\n",
    "over_test = compute_overestimation_rate(y_pred_test, true_rul_test)\n",
    "print(f\"Final Test RMSE: {rmse_test:.3f}  Overestimation rate: {over_test*100:.2f}%\")\n",
    "\n",
    "results_df = pd.DataFrame({'engine_id': test_engine_ids, 'true_RUL': true_rul_test, 'predicted_RUL': y_pred_test})\n",
    "results_df['error'] = results_df['predicted_RUL'] - results_df['true_RUL']\n",
    "results_df.to_csv('final_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3cd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Attribution (SmoothGrad Grad x Input) and mapping to subsystems\n",
    "\n",
    "def compute_grad_times_input_smooth_single(model, x_np, n_samples=12, stdev=1e-3):\n",
    "    model.eval()\n",
    "    X = torch.tensor(x_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    accum = np.zeros_like(x_np)\n",
    "    for _ in range(n_samples):\n",
    "        noise = torch.randn_like(X) * stdev\n",
    "        Xn = (X + noise).clone().detach().requires_grad_(True)\n",
    "        out = model(Xn)\n",
    "        out.sum().backward()\n",
    "        grads = Xn.grad.detach().cpu().numpy()[0]\n",
    "        inputs = Xn.detach().cpu().numpy()[0]\n",
    "        accum += np.abs(grads * inputs)\n",
    "    return accum / n_samples\n",
    "\n",
    "reports = []\n",
    "THRESHOLD_RUL_FLAG = 30\n",
    "for i, (eid, pred, true_rul) in enumerate(zip(test_engine_ids, y_pred_test, true_rul_test)):\n",
    "    attr_map = compute_grad_times_input_smooth_single(model, X_test[i])  # [seq_len, features]\n",
    "    feat_imp = attr_map.sum(axis=0)\n",
    "    feat_imp_norm = feat_imp / (feat_imp.sum() + 1e-12)\n",
    "    top_idx = feat_imp_norm.argsort()[::-1][:5]\n",
    "    top_sensors = [(feature_cols[idx], float(feat_imp_norm[idx])) for idx in top_idx]\n",
    "    # map sensors to subsystems and aggregate\n",
    "    subsystem_scores = {}\n",
    "    for idx in range(len(feature_cols)):\n",
    "        sname = feature_cols[idx]\n",
    "        sensor_key = sname  # e.g., 'sensor_1'\n",
    "        subs = SENSOR_TO_SUBSYSTEM.get(sensor_key, ['Unknown'])\n",
    "        for sub in subs:\n",
    "            subsystem_scores[sub] = subsystem_scores.get(sub, 0.0) + float(feat_imp_norm[idx])\n",
    "    # sort subsystems\n",
    "    subs_sorted = sorted(subsystem_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    imminent = bool(pred <= THRESHOLD_RUL_FLAG)\n",
    "    reports.append({\n",
    "        'engine_id': int(eid),\n",
    "        'predicted_RUL': float(pred),\n",
    "        'true_RUL': int(true_rul),\n",
    "        'imminent_failure_flag': imminent,\n",
    "        'top_sensors': top_sensors,\n",
    "        'top_subsystems': subs_sorted[:5]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(reports)\n",
    "reports_df.to_csv('engine_failure_reports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313707b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01aa6b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_engine_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === 13. Export Simple Predictions CSV (ID and RUL only) ===\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Create a simple dataframe with only engine_id and predicted_RUL\u001b[39;00m\n\u001b[32m      4\u001b[39m simple_predictions = pd.DataFrame({\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mtest_engine_ids\u001b[49m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mremaining_useful_life\u001b[39m\u001b[33m'\u001b[39m: y_pred_test.round(\u001b[32m2\u001b[39m)\n\u001b[32m      7\u001b[39m })\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[32m     10\u001b[39m simple_predictions.to_csv(\u001b[33m'\u001b[39m\u001b[33mpredictions.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_engine_ids' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 12. Visualization for LinkedIn / Reports ===\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# 12.1 True vs Predicted RUL for test engines\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(results_df['true_RUL'], results_df['predicted_RUL'], c='blue', alpha=0.6)\n",
    "plt.plot([0, MAX_RUL],[0, MAX_RUL], 'r--', label='Ideal')\n",
    "plt.xlabel(\"True RUL (cycles)\")\n",
    "plt.ylabel(\"Predicted RUL (cycles)\")\n",
    "plt.title(\"Predicted vs True RUL for Test Engines\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pred_vs_true_rul.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 12.2 Error distribution across test engines\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(results_df['error'], bins=20, kde=True, color='orange')\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "plt.xlabel(\"Prediction Error (Predicted - True RUL)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of RUL Prediction Errors\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rul_error_distribution.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 12.3 Feature importance (averaged across all engines)\n",
    "avg_feat_imp = np.zeros(len(feature_cols))\n",
    "for i in range(len(X_test)):\n",
    "    attr_map = compute_grad_times_input_smooth_single(model, X_test[i])\n",
    "    avg_feat_imp += attr_map.sum(axis=0)\n",
    "avg_feat_imp /= len(X_test)\n",
    "avg_feat_imp_norm = avg_feat_imp / (avg_feat_imp.sum() + 1e-12)\n",
    "\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': avg_feat_imp_norm\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=feat_imp_df, x='feature', y='importance', palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Feature Importance (SmoothGrad x Input, averaged over test engines)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 12.4 Subsystem importance (aggregated over engines)\n",
    "subsystem_scores = {}\n",
    "for i in range(len(X_test)):\n",
    "    attr_map = compute_grad_times_input_smooth_single(model, X_test[i])\n",
    "    feat_imp = attr_map.sum(axis=0)\n",
    "    feat_imp_norm = feat_imp / (feat_imp.sum() + 1e-12)\n",
    "    for idx, sensor_key in enumerate(feature_cols):\n",
    "        subs = SENSOR_TO_SUBSYSTEM.get(sensor_key, ['Unknown'])\n",
    "        for sub in subs:\n",
    "            subsystem_scores[sub] = subsystem_scores.get(sub, 0) + feat_imp_norm[idx]\n",
    "\n",
    "subsystem_df = pd.DataFrame(list(subsystem_scores.items()), columns=['subsystem','importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=subsystem_df, x='subsystem', y='importance', palette='magma')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Subsystem Importance (Aggregated from sensor attributions)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"subsystem_importance.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Line Graph: Predicted vs True RUL ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_engine_ids, true_rul_test, label='True RUL', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(test_engine_ids, y_pred_test, label='Predicted RUL', marker='x', linestyle='--', color='red')\n",
    "plt.xlabel(\"Engine ID\")\n",
    "plt.ylabel(\"Remaining Useful Life (cycles)\")\n",
    "plt.title(\"Predicted vs True RUL for Test Engines\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"linegraph_pred_vs_true_rul.png\", dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
